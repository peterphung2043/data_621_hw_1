---
title: "DATA 621 - Homework 1"
output:
  html_document: default
  word_document: default
date: '2022-09-10'
---

```{r  setup, knitr::all_labels(), echo=FALSE, results='hide', warning=FALSE, message=FALSE }
knitr::opts_chunk$set(echo = TRUE, class.source = "codechunk")

library(RCurl)
library(tidyr)
library(dplyr) 
library(RCurl)
library(ggplot2)
library(reshape2)
library(corrplot)
library(mice)
library(car)
library(reshape)
library(mixtools)
library(tidyverse)
suppressPackageStartupMessages(library("visdat"))
```

# Problem Statement and Goals

Our objective is to make a linear regression model that can predict how many wins a baseball team will have in a season based on certain metrics. The variables we have been provided theoretically have positive or negative effects on the total number of wins. We will be exploring this in depth in our research to figure out which variables are correlated the most strongly with the wins, as well as finding out if some of the variables can be consolidated using known conventional baseball-stats algorithms like SABER.

# Importing the datasets
```{r echo = FALSE}
#import dataset: moneyball-training-data
train<- read.csv("https://raw.githubusercontent.com/AhmedBuckets/SPS621/main/moneyball-training-data.csv")

#import dataset: moneyball-evaluation-data
eval <- read.csv("https://raw.githubusercontent.com/AhmedBuckets/SPS621/main/moneyball-evaluation-data.csv")
```

# Data Exploration

### Viewing Data

Upon first glance, the data contains 17 columns. The index column will be ignored for analysis purposes, and so that leaves the other 16. TARGET_WINS is the variable we want to investigate with regards to how well it is correlated with the other columns. To give some context, every row represents a baseball team and its performance during a particular season. TARGET_WINS is the number of wins, and each column after that represents a particular metric for the season. For example, TEAM_BATTING_H represents how many base hits by batters occurred for that team during the season. TEAM_PITCHING_E represents how many times an opposing team made a pitching mistake during the season.

```{r echo = FALSE}
train <- subset(train, select = -INDEX)
eval <- subset(eval, select = -INDEX)
summary(train)
```


<some lines about some of the features that have really high values (otential outliers that we want to look into).

Make a comment about how the target variable has an evenly distributed quartile structure, but appears normal which good...

blah blah>

### NA exploration

As can be seen below, some of the columns have missing values. Contextually, this can be possible because not every metric must have a value- for example it is possible that an entire season can be played without a batter being hit by the pitch. However it is less likely that an entire season can be played without any strikeouts by batters. We did some research and came up with ways to address each of these issues- more on that later. 

```{r echo = FALSE}
train  %>%
  summarise_all(list(~is.na(.)))%>%
  pivot_longer(everything(),
               names_to = "variables", values_to="missing") %>%
  count(variables, missing) %>%
  ggplot(aes(y=variables,x=n,fill=missing))+
  geom_col()+
  scale_fill_manual(values=c("skyblue3","gold"))+
  theme(axis.title.y=element_blank()) + theme_classic()

```

<@Peter talk about the 6 columsn that are missing values. Allude to what we will do later in section 2.

Talk about how we COULD use medians or means... but why that's bad... MICE>

### Outliers

Another question we had was one of outliers- some of the values were way too high to be realistic of a season of baseball - such as one team having over 20,000 strikeouts. 

Below we can see very quickly that some variables have extreme outliers. 

```{r echo = FALSE}
ggplot(stack(train), aes(x = ind, y = values)) +
  geom_boxplot() + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
```
Some research shows that these outliers could not make sense in a normal baseball season, for example:  

TEAM_BATTING_HITS: most hits by team in a season is 1783, so anything over should be removed or imputed
https://www.baseball-almanac.com/recbooks/hits_records_mlb_teams.shtml

There will be further discussion regarding how we dealt with the outliers on an individual variable basis.

### Data Skew

It's important to understand the distributions of each feature. Optimally, we would want to see normal distributions in order to create an effective regression model.

Creating a histogram for each of our columns, and using the `facet_wrap function` to separate each column in its own plotting panel:


```{r echo = FALSE}
train %>%
  keep(is.numeric) %>% 
  gather() %>% 
  ggplot(aes(value)) +
    facet_wrap(~ key, scales = "free") +
    geom_histogram()
```
We can see that some of the variables are skewed to the right or the left like TEAM_PITCHING_SO. Some of them even have more than one spike (bimodal) like TEAM_PITCHING_H. We will also handle these individually in the data_preparation portion. 

### Initial Correlation 

This is an initial exploration of how the variables correlate with wins. In the chart below we can see that some of these variables correlate as we would expect with the number of wins - such as TEAM_BATTING correlating positively with wins. However some of them did not make sense- like TEAM_PITCHING_SO having a negative correlation with wins. We made this chart to get a general idea of how each variable related to the number of wins.  

In this initial exploration it is clear that the outliers in some of the variables are affecting the lines of best fit. When we handle them properly, as well as impute the missing data, these lines will likely change. 

```{r echo = FALSE}
bb_games_melted <- melt(train, "TARGET_WINS")
ggplot(data = bb_games_melted, aes(value, TARGET_WINS)) +
  geom_point() +
  facet_wrap(.~variable, scales = "free") +
  geom_smooth(method = "lm")

```

### Examining Feature Multicollinearity h

```{r}
corrplot(cor(train, use = "na.or.complete"), method = 'number', type = 'lower', diag = FALSE, number.cex = 0.5, tl.cex = 0.5)
```

< Add text about the features that are highly correlated with eachother. Maybe choose like 3 or 4 and discuss why they either make sense or don't make sense. Allude to how we will use this information in sectinn 3 when we build our models>

@Peter double check what the output is for coffy's corrplot and see if the same features are correlated

# Data Preperation

### Renaming Column Names

Keeping column names short and readable is important in order to practice ["table hygiene"](https://dataindependent.com/pandas/pandas-change-column-names-3-methods/).
Therefore, new column names were generated and are shown on Table XX.

```{r table1, echo=FALSE, message=FALSE, warnings=FALSE, results='asis'}
tabl <- "
| Original Column Name|New Column Name|
|---------------|-------------:|
|TARGET_WINS|target|
|TEAM_BATTING_H|bat_h|
|TEAM_BATTING_2B|bat_2b|
|TEAM_BATTING_3B|bat_3b|
|TEAM_BATTING_HR|bat_hr|
|TEAM_BATTING_BB|bat_bb|
|TEAM_BATTING_HBP|bat_hbp|
|TEAM_BATTING_SO|bat_so|
|TEAM_BASERUN_CS|bas_cs|
|TEAM_FIELDING_E|f_e|
|TEAM_FIELDING_DP|f_dp|
|TEAM_PITCHING_BB|p_bb|
|TEAM_PITCHING_H|p_h|
|TEAM_PITCHING_HR|p_hr|
|TEAM_PITCHING_SO|p_so|
"
cat(tabl) # output the table in a format good for HTML/PDF/docx conversion
```

```{r New column names for training data, echo = FALSE}

# run this for training data

new_cols <- c("target", "bat_h", "bat_2b", "bat_3b", "bat_hr", "bat_bb", "bat_so", "bas_sb", "bas_cs", "bat_hbp", "p_h", "p_hr", "p_bb", "p_so", "f_e", "f_dp"
)

colnames(train) <- new_cols
```

```{r New column names for evaluation data, echo = FALSE}

# run this for evaluation data

new_cols <- c("bat_h", "bat_2b", "bat_3b", "bat_hr", "bat_bb", "bat_so", "bas_sb", "bas_cs", "bat_hbp", "p_h", "p_hr", "p_bb", "p_so", "f_e", "f_dp"
)

colnames(eval) <- new_cols
```

### Dealing with Missing Values

As shown in section 1, there are 6 features that have missing values:

- Strikeouts by batters (5%): Should use median or regression model for imputation

- Stolen bases (6%): Stolen bases weren’t tracked officially until 1887, which means some of the missing data could be from 1871-1886. These values could be imputed.

- Caught stealing (34%): Stolen bases weren’t tracked officially until 1887, so some of the missing data could be from 1871-1886. These values could be imputed.

- Batter hit by pitch (92%): This predictor will be removed from the analysis as too many of its values are missing.

- Strikeouts by pitchers (4%): Should use median or regression model for imputation

- Double plays (12%): Should use median or regression model for imputation


---------------------------------

[Tabachnick and Fidell ](https://imaging.mrc-cbu.cam.ac.uk/statswiki/FAQ/missing) point out that imputations by the means/medians is acceptable if the missing values only account for 5% of the sample. Peng et al.(2006) suggest that mean imputation is permissible only if no more than 20% of the data is missing. [Bruin](https://stats.oarc.ucla.edu/sas/seminars/multiple-imputation-in-sas/mi_new_1/) points out that unconditional mean and median imputation results in an artificual reduction in variability due to the fact that values are being imputed at the center of the variable's distribution. [Wicklin](https://blogs.sas.com/content/iml/2017/12/06/problems-mean-imputation.html) points out that: 


"...mean and median imputation also shrinks standard errors, which invalidates most hypothesis tests and the calculation of the confidence interval." 

and


"...does not preserve relationships between variables such as correlations."

The MICE package in R implements a methodology where each incomplete variable is imputed by a separate model. [Alice](https://datascienceplus.com/imputing-missing-data-with-r-mice-package/) points out that plausible values are drawn from a distribution specifically designed for each missing datapoint. Many imputation methods can be used within the package. The one that was selected for the data being analyzed in this report is PMM (Predictive Mean Matching), which is used for quantitative data. 

[Van Buuren](https://stefvanbuuren.name/fimd/sec-pmm.html) explains that PMM works by selecting values from the observed/already existing data that would most likely belong to the variable in the observation with the missing value. The advantage of this is that it selects values that must exist from the observed data, so no negative values will be used to impute missing data.Not only that, it circumvents the shrinking of errors by using multiple regression models. The variability between the different imputed values gives a wider, but more correct standard error. Uncertainty is inherent in imputation which is why having multiple imputed values is important. Not only that. [Marshall et al. 2010](https://stefvanbuuren.name/fimd/sec-pmm.html) points out that:

"Another simulation study that addressed skewed data concluded that predictive mean matching 'may be the preferred approach provided that less than 50% of the cases have missing data...'

---------------------------------

The density of the imputed data for each imputed dataset is shown in magenta. The density of the observed data is shown in blue



```{r Using MICE, echo = FALSE}
# Removal of bat_hbp
train <- subset(train, select = -c(bat_hbp))
eval <- subset(eval, select = -c(bat_hbp))
```

```{r Imputing the missing data from MICE, include = FALSE}
temp_data <- mice(train,m=4,maxit=5,meth='midastouch',seed=500)
temp_eval_data <- mice(eval,m=3,maxit=5,meth='pmm',seed=500)
# summary(temp_data)
```

```{r, echo = FALSE}
complete_data <- complete(temp_data,1)
complete_eval_data <- complete(temp_eval_data,1)
```

```{r}
densityplot(temp_data)
```

### Analysis of Outliers

Several predictors contained outliers that contradicted existing baseball statistics or fell out of an "acceptable" range given the features inherent distribution. These features are:

- bat_h: The most hits by team in a season is 1783. Therefore, any values above 1,783 were replaced with the median for the predictor [(Source)](https://www.baseball-almanac.com/recbooks/hits_records_mlb_teams.shtml).

```{r, echo = FALSE}
replace_median <- median(complete_data$bat_h[complete_data$bat_h <= 1783])

complete_data$bat_h[complete_data$bat_h > 1783] <- replace_median

complete_eval_data$bat_h[complete_eval_data$bat_h > 1783] <- replace_median
```


- p_h: We could not find any suitable statistics from outside sources for this feature. However, we can apply interquartile outlier analysis. By analyzing a given feature, those datapoints which fall above or below an "acceptable" range can be identified given the features inherent distribution.


```{r, echo = FALSE}
Q1 <- quantile(complete_data$p_h, probs=.25)
Q3 <- quantile(complete_data$p_h, probs=.75)
iqr = Q3-Q1
upper_limit = Q3 + (iqr*1.5)
lower_limit = Q1 - (iqr*1.5)
replace_median <- median(complete_data$p_h[(complete_data$p_h < upper_limit) | (complete_data$p_h > lower_limit)])

complete_data$p_h[(complete_data$p_h > upper_limit) | (complete_data$p_h < lower_limit)] <- replace_median

complete_eval_data$p_h[(complete_eval_data$p_h > upper_limit) | (complete_eval_data$p_h < lower_limit)] <- replace_median

```


- p_so: The record for most strikeouts in a season is 1595. Anything above this should be removed or imputed [(Source)](https://www.baseball-almanac.com/recbooks/rb_strike2.shtml).

```{r, echo = FALSE}
replace_median <- median(complete_data$p_so[complete_data$p_so <= 1595])

complete_data$p_so[complete_data$p_so > 1595] <- replace_median

complete_eval_data$p_so[complete_eval_data$p_so > 1595] <- replace_median
```

- f_e: The record for most errors in a season is 886. Anything above this should be removed or imputed [(Source)](https://www.baseball-fever.com/forum/general-baseball/statistics-analysis-sabermetrics/2403-team-errors-in-a-season). 

```{r, echo = FALSE}
replace_median <- median(complete_data$f_e[complete_data$f_e <= 886])

complete_data$f_e[complete_data$f_e > 886] <- replace_median

complete_eval_data$f_e[complete_eval_data$f_e > 886] <- replace_median
```

- p_bb: We could not find any suitable statistics from outside sources for this feature. However, we can apply interquartile outlier analysis. By analyzing a given feature, those datapoints which fall above or below an "acceptable" range can be identified given the features inherent distribution.

```{r, echo = FALSE}
Q1 <- quantile(complete_data$p_bb, probs=.25)
Q3 <- quantile(complete_data$p_bb, probs=.75)
iqr = Q3-Q1
upper_limit = Q3 + (iqr*1.5)
lower_limit = Q1 - (iqr*1.5)
replace_median <- median(complete_data$p_bb[(complete_data$p_bb < upper_limit) | (complete_data$p_bb > lower_limit)])

complete_data$p_bb[(complete_data$p_bb > upper_limit) | (complete_data$p_bb < lower_limit)] <- replace_median

complete_eval_data$p_bb[(complete_eval_data$p_bb > upper_limit) | (complete_eval_data$p_bb < lower_limit)] <- replace_median

```

#### Updated Distributions After Outlier Analysis and Imputing NA Values

```{r echo = FALSE}
ggplot(stack(complete_data), aes(x = ind, y = values, fill=ind)) +
  geom_boxplot() + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
```

< THIS LOOKS MUCH BETTER, THERE ARE STILL SOME PROBLEMS WITH FEATURES LIKE BAS_SB, F_E... WE CAN CONSIDER ADDRESSING THESE AT A LATER TIME (BUT WILL NOT DO IT!!!!!!)>

### Box-Cox Transformation for skewed variables

<Remind the reader which features had skews>

Try replacing the 0 values with really small values(1e-6) which will allow you to perform the Box-Cox transformation.

```{r}
complete_data[complete_data == 0] <- 1e-6

complete_eval_data[complete_eval_data == 0] <- 1e-6
```

After we use `powerTransform` to do the Box-Cox transformation, we than delete the original columns from `complete_data` using the `select` function from dplyr. Than use the `cbind` function to append the `transformed_data` to the `complete_data`. 

```{r}

skewed_vars <- "bat_3b, bas_sb, bas_cs, f_e"

lambdas <- powerTransform(eval(parse(text = paste("cbind(",skewed_vars,")", "~ 1"))), complete_data)

transformed_data <- bcPower(lambdas$y, coef(lambdas))

complete_data <- cbind(subset(complete_data, select = eval(parse(text = paste("-c(", skewed_vars, ")")))),
                       transformed_data)
```

```{r}

lambdas <- powerTransform(eval(parse(text = paste("cbind(",skewed_vars,")", "~ 1"))), complete_eval_data)

transformed_data <- bcPower(lambdas$y, coef(lambdas))
colnames(transformed_data) <- colnames(lambdas$y)

complete_eval_data <- cbind(subset(complete_eval_data, select = eval(parse(text = paste("-c(", skewed_vars, ")")))),transformed_data)
```


```{r echo = FALSE}
complete_data %>%
  keep(is.numeric) %>% 
  gather() %>% 
  ggplot(aes(value)) +
    facet_wrap(~ key, scales = "free") +
    geom_histogram()
```

### Dealing with Bimodal Variables

<@Peter please write an explanation for your approach>

```{r}
# Finds where two histograms intersect
histogram_intersection <- function(mu_1, mu_2, sigma_1, sigma_2){
  if (sigma_1 == sigma_2) stop('Both Sigmas are the same. Get 1/0')
  (mu_2*(sigma_1^2) - sigma_2*(mu_1*sigma_2 + sigma_1*sqrt((mu_1 - mu_2)^2 + 2*(sigma_1^2 -      sigma_2^2)*log(sigma_1/sigma_2))))/(sigma_1^2 - sigma_2^2)
}

# Fits two histograms to df[,bimodal_var] where `bimodal_var` is a bimodal
# variable. Than finds the point where the two histograms intersects. This
# value is returned as `cutoff`
create_bimodal_cutoff <- function(bimodal_var, df){
  bimodal_var_data <-  df[,bimodal_var]
  mixmdl = normalmixEM(bimodal_var_data)
  
  mu_1 = mixmdl$mu[1]
  mu_2 = mixmdl$mu[2]
  
  sigma_1 = mixmdl$sigma[1]
  sigma_2 = mixmdl$sigma[2]
  
  cutoff <- histogram_intersection(mu_1, mu_2, sigma_1, sigma_2)
  
  plot(mixmdl,which=2)
  lines(density(bimodal_var_data), lty=2, lwd=2)
  abline(v = cutoff)
  return(cutoff)
}

# Creates a dummy variable where any values for df[,bimodal_var] below `cutoff`
# are given a 1, and any values above are given a 0. Since these are dummy
# variables, they are converted from `numeric` to `factor`s using the `factor`
# function.
append_bimodal_dummy_var <- function(cutoff, bimodal_var, df){
  df[,paste("bi", bimodal_var, sep = "_")] <- factor((df[,bimodal_var] < cutoff) * 1)
  return(df)
}

# Creates dummy variables based on bimodal data. 
create_bimodal_dummy_var <- function(bimodal_var, df){
  cutoff <- create_bimodal_cutoff(bimodal_var, df)
  df <- append_bimodal_dummy_var(cutoff, bimodal_var, df)
  return(df)
}

```

```{r}
for (bimodal_var in c("bat_so", "p_hr", "bat_hr")){
  complete_data <- create_bimodal_dummy_var(bimodal_var, complete_data)
}
```

```{r eval=FALSE}
ggplot(data = melt(complete_data, "target"), aes(value)) +
  geom_histogram() +
  facet_wrap(.~variable, scales = "free")
```


--------------------------- PETER TO CORRECT / CREATE FUNCTION


### Add SABER analysis

Saber Model

Sabermetrics has become the rage in baseball, actually popularized by Billy Beane and the data set we are exploring. As a result, we built a model that centers around one of these advance analytics known as BsR or base runs. This statistic (designed by David Smyth in the 1990’s) estimates the amount of runs a team SHOULD score, adding an intriguing element to a data set which does not include runs (see http://tangotiger.net/wiki_archive/Base_Runs.html for more information). The formula For BsR is as follows:

BSR = A*B/(B+A)+C where:

A = TEAM_BATTING_1B + TEAM_BATTING_2B + TEAM_BATTING_3B + TEAM_BATTING_BB

B = 1.02(1.4TEAM_TOTAL_BASES -0.6TEAM_BATTING_H + 0.1TEAM_BATTING_BB)

C = TEAM_BATTING_HR

Since we eliminate the value of TEAM_BATTING_H, we sum up singles, doubles, triples and home runs in the actual code, and the approach for TEAM_TOTAL_BASES is described in model 2. The data for BSR exhibit a fairly normal distribution.


```{r}
complete_data$bat_1b <- complete_data$bat_h - complete_data$bat_2b - complete_data$bat_3b - complete_data$bat_hr
complete_data$total_bases <- complete_data$bat_1b + 2*complete_data$bat_2b + 3*complete_data$bat_3b + 4*complete_data$bat_hr


A <- complete_data$bat_h
B <- 1.02*(1.4*complete_data$total_bases -0.6*complete_data$bat_h + 0.1*complete_data$bat_bb)
C <- complete_data$bat_hr

complete_data$saber <- A*B/(B+A)+C
```

```{r}
complete_eval_data$bat_1b <- complete_eval_data$bat_h - complete_eval_data$bat_2b - complete_eval_data$bat_3b - complete_eval_data$bat_hr

complete_eval_data$total_bases <- complete_eval_data$bat_1b + 2*complete_eval_data$bat_2b + 3*complete_eval_data$bat_3b + 4*complete_eval_data$bat_hr


A <- complete_eval_data$bat_h
B <- 1.02*(1.4*complete_eval_data$total_bases -0.6*complete_eval_data$bat_h + 0.1*complete_eval_data$bat_bb)
C <- complete_eval_data$bat_hr

complete_eval_data$saber <- A*B/(B+A)+C
```

```{r}
#write.csv(complete_data, "../data/final_clean_train.csv")
```

```{r}
#write.csv(complete_eval_data, "../data/final_clean_eval.csv")
```

# Build Models

```{r eval-load, eval=FALSE}
#import dataset: moneyball_evaluation_data
original <- read_csv("./data/moneyball-training-data.csv")
original <- subset(original, select = -INDEX)
train <- read_csv("./data/final_clean_train.csv")
train <- train[-1]
eval <- read_csv("./data/final_clean_eval.csv")
eval <- eval[-1]
```


```{r}
data <- train
```

```{r}

# run this for training data

new_cols <- c("target", "bat_h", "bat_2b", "bat_3b", "bat_hr", "bat_bb", "bat_so", "bas_sb", "bas_cs", "bat_hbp", "p_h", "p_hr", "p_bb", "p_so", "f_e", "f_dp"
)

colnames(original) <- new_cols
```


#### Create tracker dataframe

<Create an empty dataframe with 2 columns:
- model name
- model Adjusted R^2
- create a utility function that automatically updates the table

```{r}
tracker <- as.data.frame(matrix(ncol=2))
colnames(tracker) <- c("Model", "Adjusted R-Squared")
```


### Examine base model, no transformations, no engineering

<manually make a model using the clean training data, and hand picking only the original columns>

<we can do a subset and exclude all of the new columns>

```{r}
base <- train %>% select("target", "bat_h", "bat_2b", "bat_hr", "bat_bb", "bat_so", "bat_hbp", "p_h", "p_hr", "p_bb", "p_so", "f_dp", "bat_3b^0.39", "bas_sb^0.24", "bas_cs^0.23", "f_e^-0.76")
```


remove bat_1b, total_bases, saber (all columns not in original data)

```{r}
base_mdl <- lm(target~., data=base)
```

<Here add the model summary, so that we can compare against when we include SABER>

```{r}
base_sum <-  summary(base_mdl)
tracker[nrow(tracker) + 1,] <- c("Base Model", base_sum$adj.r.squared)
```


<Add comments>

### Evaluate SABER model

```{r}
mdl_inc_saber <- lm(target~., data=data)
sum_saber <- summary(mdl_inc_saber)
tracker[nrow(tracker) + 1,] <- c("Saber Model", sum_saber$adj.r.squared)
```

<Add comments>

## First Model - SABER reduced

<We need to examine the summary from above (SABER model) and pick all features that have high significance>

<Results might be worse, but the model will be more parsimonious / easily explainable>

<Add comments>


### Ahmeds "Human Intuition Model"

<Write code to edit column names to make sure it matches alec's column names>

<Investigate what the ...1 is>


```{r}
model <- lm(target ~ bat_h + bat_2b + bat_3b + bat_so + bas_sb + p_h + f_e, data = data)

summary(model)
```

<Add comments>

### Step AIC

<make sure to connect base model to this>

```{r}
mdl_step.model <- stepAIC(mdl, direction = "both", 
                      trace = FALSE)
summary(mdl_step.model)
```

<Add comments>

### Square Root Step AIC

```{r}
mdl_sqrt = lm(sqrt(target) ~ .,
               data = data)
summary(mdl_sqrt)
```

```{r}
mdl_sqrt_step.model <- stepAIC(mdl_sqrt, direction = "both", 
                      trace = FALSE)
summary(mdl_sqrt_step.model)
```

<Add comments>

### All models

```{r}
mdl_all <- olsrr::ols_step_all_possible(model)
```

<Add comments>


