---
title: "Homework 3"
output:
  pdf_document: default
  html_document: default
date: '2022-11-01'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, comment = NA)
```

```{r}
#alternative to install or load R packages
install_or_load_pack <- function(pkg) {
  create.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
  
  if (length(create.pkg))
    install.packages(create.pkg, dependencies = TRUE)
  sapply(pkg, require, character.only = TRUE)
}
```

```{r}
#required libraries
packages <- c("RCurl", "tidyr", "dplyr", "reshape2", "corrplot", "mice", "car", "reshape", "mixtools", "tidyverse", "GGally", "MASS", "regclass")
```

```{r eval=FALSE}
#initial version on loading libraries
library(RCurl)
library(tidyr)
library(dplyr) 
library(RCurl)
library(ggplot2)
library(reshape2)
library(corrplot)
library(mice)
library(car)
library(reshape)
library(mixtools)
library(tidyverse)
library(GGally)
library(MASS)
library(regclass)
```

## Importing Data

```{r}
#load data sets from GitHub repository
crime_eval <- read.csv("https://raw.githubusercontent.com/peterphung2043/data_621_hw_1/main/HW3/crime-evaluation-data_modified.csv")
crime_training <- read.csv("https://raw.githubusercontent.com/peterphung2043/data_621_hw_1/main/HW3/crime-training-data_modified.csv")

crime_training[crime_training == 0.0] = 1e-6
```

## Problem Statement and Goals

In this report, we generate a binary logistic regression model that is able to predict whether or not the crime rate for a neighborhood is above the median crime rate (1) or not (0). The independent and dependent variables that are used in order to generate this model use data from various neighborhoods of a major city. The analysis detailed in this report shows the testing of several models from which a best model was selected based on model performance and various metrics. 

## Data Exploration

The following is a summary of the variables provided within the data to generate the binary logistic regression model:

- `zn`: proportion of residential land zoned for large lots (over 25000 square feet) (predictor variable)
- `indus`: proportion of non-retail business acres per suburb (predictor variable)
- `chas`: a dummy var. for whether the suburb borders the Charles River (1) or not (0) (predictor variable)
- `nox`: nitrogen oxides concentration (parts per 10 million) (predictor variable)
- `rm`: average number of rooms per dwelling (predictor variable)
- `age`: proportion of owner-occupied units built prior to 1940 (predictor variable)
- `dis`: weighted mean of distances to five Boston employment centers (predictor variable)
- `rad`: index of accessibility to radial highways (predictor variable)
- `tax`: full-value property-tax rate per $10,000 (predictor variable)
- `ptratio`: pupil-teacher ratio by town (predictor variable)
- `lstat`: lower status of the population (percent) (predictor variable)
- `medv`: median value of owner-occupied homes in $1000s (predictor variable)
- `target`: whether the crime rate is above the median crime rate (1) or not (0) (response variable)

A summary of the variables is shown below. See that within the summary, there does not seem to be any extremely high or extremely low values relative to the medians and means for each of the continuous predictor variables. The single binary predictor variable `chas` has reasonable values as well.


**alternative on viewing the data set variables for descriptive statistics**
```{r}
#view data set variables summary statistics
summary(crime_training)
```

**converting the variables class can be performed during later**
```{r, comment = NA}
#view data set variables summary statistics
crime_training$chas <- factor(crime_training$chas)
crime_training$target <- factor(crime_training$target)

summary(crime_training)
```

Figure XX reveals that there are no missing values within the dataset. Therefore, no imputing is required for this dataset.

```{r}
crime_training  %>%
  summarise_all(list(~is.na(.)))%>%
  pivot_longer(everything(),
               names_to = "variables", values_to="missing") %>%
  count(variables, missing) %>%
  ggplot(aes(y=variables,x=n,fill=missing))+
  geom_col()+
  scale_fill_manual(values=c("skyblue3","gold"))+
  theme(axis.title.y=element_blank()) + theme_classic()
```
*Figure XX: Chart showing the count of missing values for each of the variables in the dataset. Note that since there are no missing values, the legend only shows one item.*

### Outliers

```{r echo = FALSE}
par(mfrow = c(4, 3), mai = c(0.1, 0.6, 0.1, 0.1))

for (col_name in colnames(crime_training %>% dplyr::select(-chas, -target))){
  boxplot(crime_training[[col_name]],
          ylab = col_name)
}
```
*Figure XX: Box plots for each of the variables in the dataset.*


Figure XX shows boxplots for the continuous variables. While `zn`, `rm`, `dis`, `lstat` and `medv` contain outliers, the outliers in general do not seem to be any significant enough to affect the model greatly. However. note that  `rad` and `tax` have significantly large interquartile ranges which indicates skewness.


```{r echo = FALSE}
crime_training %>%
  keep(is.numeric) %>% 
  gather() %>% 
  ggplot(aes(value)) +
    facet_wrap(~ key, scales = "free") +
    geom_density()
```
*Figure XX: Density plots for continuous variables*

Figure xx reveals that `tax`, `indus`, and `rad` have bimodality. `age` appears to have bimodality as well but it is not as pronounced as the others. `rm` is relatively normally distributed while all of the other variables possess skewness, with `zn` possessing extreme skewness. Dummy variables for each of the bimodal variables were created and are given an explanation in the "Dealing with Bimodal Variables" section.

## Simple Model

A simple model was generated using all of the predictors and served as a baseline to which the other models were compared against.

```{r}
simple_log_reg <- glm(target ~ ., data = crime_training, family = binomial(link = "logit"))
summary(simple_log_reg)
```

## Goodness of Fit Tests for Simple Model

```{r}
check_regression(simple_log_reg)
```


`nox` has an extremely low P-value. The U.S. Department of Housing indicates that low-income communities are much more likely than others to experience crime. The National Institute of Environmental Health Sciences indicates that poor communities are exposed to elevated pollution levels, which probably explains why there `nox` is statistically significant. Note that the `lstat`, `indus`, `rm` variables have a p-value greater than 0.05. We reasoned that if the skewed variables were transformed to a normal distribution, than the p-values could decrease, but the p-values actually increased further, thus negating the need to transform the variables.

```{r}
par(mfrow = c(2, 2))
plot(simple_log_reg)
```

Cook's distance was used to identify observations within the dataset that negatively affect the regression model. Figure XX reveals that for the simple model, none of the observations exceed the dashed red lines, indicating that there are no influential datapoints within the dataset when using the simple model.

### ANOVA Chi Square Test and VIF Calculation for Simple Linear Model

An ANOVA Chi-square test can be used in order to check the overall effect of variables on the dependent variable. The output below shows that `chas`, `age`, and `lstat` are not statistically significant. However, while `chas` and `lstat` were not significant in the simple model, the variable `age` was. Therefore, we leave these variables as is and perform elimination. The VIF was also calculated for all of the variables in the simple linear model. A general rule of thumb is that a VIF of 5 indicates severe multicollinearity which `rm` and `medv` have. Therefore, `rm` and `medv` were removed from the model.


```{r}
anova(simple_log_reg, test = 'Chisq')
```

```{r}
car::vif(simple_log_reg)
```

### Examining Feature Multicollinearity

Finally, it is imperative to understand which features are correlated with each other in order to address and avoid multicollinearity within our models. By using a correlation plot, we can visualize the relationships between certain features. The correlation plot is only able to determine the correlation for continuous variables. There are methodologies to determine correlations for categorical variables (tetrachoric correlation). However there is only one binary predictor variable which is why the multicollinearity will only be considered for the continuous variables.


```{r}
corrplot(cor(subset(crime_training, select = -c(chas, target)), use = "na.or.complete"),
         method = 'number',
         type = 'lower',
         diag = FALSE,
         number.cex = 0.75,
         tl.cex = 0.5)
```
*Figure xx: Multicollinearity plot for continuous predictor variables*

Figure XX reveals that `rad` and `tax` have an extremely high correlation of 0.91. What this indicates that there is a significant correlation between access to radial highways and property taxes. Therefore, the `tax` variable should be removed from the dataset because it has a higher p-value in the simple model to mitigate this high degree of correlation.

```{r}
crime_training <- subset(crime_training, select = -c(tax, rm, medv))
```

### Dealing with Bimodal Variables

Bimodal distributions in data are interesting, in that they represent features which actually contain multiple (2) inherent systems resulting in separated distributional peaks. Our approach to solving this is to create dummy variables representing which side of the local minimum each datapoint falls with respect to it's original bimodal distribution. Two new dummy variables were created for the two bimodal variables (`bi_indus` and `bi_rad`). The algorithm that was written to determine the local minimum was able to determine the local minimum for `indus` to be 12.70692. The algorithm was unable to detect a local minimum for `rad`. There is probably not enough information for the right peak for the algorithm to work properly. Nevetheless, we determined that a cutoff value of 15 for this variable would suffice. To summarize:

- `bi_indus`: 1 if `indus` is greater than 12.70692, 0 otherwise.
- `bi_rad`: 1 if `rad` is greater than 15, 0 otherwise.

```{r}
# Finds where two histograms intersect
histogram_intersection <- function(mu_1, mu_2, sigma_1, sigma_2){
  if (sigma_1 == sigma_2) stop('Both Sigmas are the same. Get 1/0')
  (mu_2*(sigma_1^2) - sigma_2*(mu_1*sigma_2 + sigma_1*sqrt((mu_1 - mu_2)^2 + 2*(sigma_1^2 - sigma_2^2)*log(sigma_1/sigma_2))))/(sigma_1^2 - sigma_2^2)
}
# Fits two histograms to df[,bimodal_var] where `bimodal_var` is a bimodal
# variable. Than finds the point where the two histograms intersects. This
# value is returned as `cutoff`
create_bimodal_cutoff <- function(bimodal_var, df){
  bimodal_var_data <-  df[,bimodal_var]
  mixmdl = normalmixEM(bimodal_var_data)
  
  mu_1 = mixmdl$mu[1]
  mu_2 = mixmdl$mu[2]
  
  sigma_1 = mixmdl$sigma[1]
  sigma_2 = mixmdl$sigma[2]
  
  cutoff <- histogram_intersection(mu_1, mu_2, sigma_1, sigma_2)
  print(cutoff)
  
  hist(bimodal_var_data, freq = FALSE, main = paste("Histogram and Density Plot of" , bimodal_var))
  lines(density(bimodal_var_data), lty = 2, lwd = 2)
  abline(v = cutoff, col = "red", lty = 2, lwd = 3)
  return(cutoff)
}
# Creates a dummy variable where any values for df[,bimodal_var] below `cutoff`
# are given a 1, and any values above are given a 0. Since these are dummy
# variables, they are converted from `numeric` to `factor`s using the `factor`
# function.
append_bimodal_dummy_var <- function(cutoff, bimodal_var, df){
  df[,paste("bi", bimodal_var, sep = "_")] <- factor((df[,bimodal_var] > cutoff) * 1)
  return(df)
}
# Creates dummy variables based on bimodal data. 
create_bimodal_dummy_var <- function(bimodal_var, df){
  cutoff <- create_bimodal_cutoff(bimodal_var, df)
  df <- append_bimodal_dummy_var(cutoff, bimodal_var, df)
  return(df)
}
```

```{r}
for (bimodal_var in c("indus")){
  crime_training <- create_bimodal_dummy_var(bimodal_var, crime_training)
}
crime_training <- append_bimodal_dummy_var(15, "rad", crime_training)
```

### Skewed Variables

A Modern Approach to Regression with R explains the following:

*When conducting a binary regression with a skewed predictor, it is often easiest to assess the
need for x and log( x ) by including them both in the model so that their relative contributions
can be assessed directly.*

The variables, `lstat`, `dis`, `age`, `nox`, and `ptratio` all exhibit skewness. Therefore, the logs of these variables were added into the dataset.

```{r}
target_variables <- c("lstat", "dis", "age", "nox", "ptratio")

for (target_var in target_variables){
  crime_training[,paste("log", target_var, sep = "_")] <- log(crime_training[target_var])
}
```


## Works Cited

[1] What is Cook's Distance? (StatisticsHowTo): https://www.statisticshowto.com/cooks-distance/

[2] How to Calculate Correlation Between Categorical Variables (Statology): https://www.statology.org/correlation-between-categorical-variables/

[3] The 6 Assumptions of Logistic Regression (Statology): https://www.statology.org/assumptions-of-logistic-regression/

[4] Neighborhoods and Violent Crime (U.S. Department of Housing) https://www.huduser.gov/portal/periodicals/em/summer16/highlight2.html

[5] Poor Communities Exposed to Elevated Air Pollution Levels
https://www.niehs.nih.gov/research/programs/geh/geh_newsletter/2016/4/spotlight/poor_communities_exposed_to_elevated_air_pollution_levels.cfm

[6] https://github.com/kennethleungty/Logistic-Regression-Assumptions/blob/main/Box-Tidwell-Test-in-R.ipynb

[7] Logistic Regression Assumptions and Diagnostics in R (STHDA): http://www.sthda.com/english/articles/36-classification-methods-essentials/148-logistic-regression-assumptions-and-diagnostics-in-r/