---
output:
  pdf_document: default
  html_document: default
---

## Data Preparation

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(list = ls())
if(!is.null(dev.list()))dev.off()
```


```{r, include = FALSE}
library(mice)
library(car)
library(reshape)
library(mixtools)
```


```{r, echo = FALSE}
suppressPackageStartupMessages(library(tidyverse))
```


```{r eval-load, echo=FALSE}
#import dataset: moneyball_evaluation_data
eval <- read.csv("../data/moneyball-evaluation-data.csv")
eval <- subset(eval, select = -INDEX)
```

```{r train-load, echo=FALSE}
#import dataset: moneyball_training_data
train <- read.csv("../data/moneyball-training-data.csv")
train <- subset(train, select = -INDEX)
```


### Renaming Column Names

Keeping column names short and readable is important in order to practice ["table hygiene"](https://dataindependent.com/pandas/pandas-change-column-names-3-methods/).
Therefore, new column names were generated and are shown on Table XX.

```{r table1, echo=FALSE, message=FALSE, warnings=FALSE, results='asis'}
tabl <- "
| Original Column Name|New Column Name|
|---------------|-------------:|
|TARGET_WINS|target|
|TEAM_BATTING_H|bat_h|
|TEAM_BATTING_2B|bat_2b|
|TEAM_BATTING_3B|bat_3b|
|TEAM_BATTING_HR|bat_hr|
|TEAM_BATTING_BB|bat_bb|
|TEAM_BATTING_HBP|bat_hbp|
|TEAM_BATTING_SO|bat_so|
|TEAM_BASERUN_CS|bas_cs|
|TEAM_FIELDING_E|f_e|
|TEAM_FIELDING_DP|f_dp|
|TEAM_PITCHING_BB|p_bb|
|TEAM_PITCHING_H|p_h|
|TEAM_PITCHING_HR|p_hr|
|TEAM_PITCHING_SO|p_so|
"
cat(tabl) # output the table in a format good for HTML/PDF/docx conversion
```

```{r New column names for training data, echo = FALSE}

# run this for training data

new_cols <- c("target", "bat_h", "bat_2b", "bat_3b", "bat_hr", "bat_bb", "bat_so", "bas_sb", "bas_cs", "bat_hbp", "p_h", "p_hr", "p_bb", "p_so", "f_e", "f_dp"
)

colnames(train) <- new_cols
```

```{r New column names for evaluation data, echo = FALSE}

# run this for evaluation data

new_cols <- c("bat_h", "bat_2b", "bat_3b", "bat_hr", "bat_bb", "bat_so", "bas_sb", "bas_cs", "bat_hbp", "p_h", "p_hr", "p_bb", "p_so", "f_e", "f_dp"
)

colnames(eval) <- new_cols
```


### Dealing with Missing Values

As shown in section 1, there are 6 features that have missing values:

- Strikeouts by batters (5%): Should use median or regression model for imputation

- Stolen bases (6%): Stolen bases weren’t tracked officially until 1887, which means some of the missing data could be from 1871-1886. These values could be imputed.

- Caught stealing (34%): Stolen bases weren’t tracked officially until 1887, so some of the missing data could be from 1871-1886. These values could be imputed.

- Batter hit by pitch (92%): This predictor will be removed from the analysis as too many of its values are missing.

- Strikeouts by pitchers (4%): Should use median or regression model for imputation

- Double plays (12%): Should use median or regression model for imputation


---------------------------------

[Tabachnick and Fidell ](https://imaging.mrc-cbu.cam.ac.uk/statswiki/FAQ/missing) point out that imputations by the means/medians is acceptable if the missing values only account for 5% of the sample. Peng et al.(2006) suggest that mean imputation is permissible only if no more than 20% of the data is missing. [Bruin](https://stats.oarc.ucla.edu/sas/seminars/multiple-imputation-in-sas/mi_new_1/) points out that unconditional mean and median imputation results in an artificual reduction in variability due to the fact that values are being imputed at the center of the variable's distribution. [Wicklin](https://blogs.sas.com/content/iml/2017/12/06/problems-mean-imputation.html) points out that: 


"...mean and median imputation also shrinks standard errors, which invalidates most hypothesis tests and the calculation of the confidence interval." 

and


"...does not preserve relationships between variables such as correlations."

The MICE package in R implements a methodology where each incomplete variable is imputed by a separate model. [Alice](https://datascienceplus.com/imputing-missing-data-with-r-mice-package/) points out that plausible values are drawn from a distribution specifically designed for each missing datapoint. Many imputation methods can be used within the package. The one that was selected for the data being analyzed in this report is PMM (Predictive Mean Matching), which is used for quantitative data. 

[Van Buuren](https://stefvanbuuren.name/fimd/sec-pmm.html) explains that PMM works by selecting values from the observed/already existing data that would most likely belong to the variable in the observation with the missing value. The advantage of this is that it selects values that must exist from the observed data, so no negative values will be used to impute missing data.Not only that, it circumvents the shrinking of errors by using multiple regression models. The variability between the different imputed values gives a wider, but more correct standard error. Uncertainty is inherent in imputation which is why having multiple imputed values is important. Not only that. [Marshall et al. 2010](https://stefvanbuuren.name/fimd/sec-pmm.html) points out that:

"Another simulation study that addressed skewed data concluded that predictive mean matching 'may be the preferred approach provided that less than 50% of the cases have missing data...'

---------------------------------

The density of the imputed data for each imputed dataset is shown in magenta. The density of the observed data is shown in blue


```{r Using MICE, echo = FALSE}
# Removal of bat_hbp
train <- subset(train, select = -c(bat_hbp))
eval <- subset(eval, select = -c(bat_hbp))
```

```{r Imputing the missing data from MICE, include = FALSE}
# temp_data <- mice(train,m=4,maxit=5,meth='midastouch',seed=500)
temp_eval_data <- mice(eval,m=3,maxit=5,meth='pmm',seed=500)
# summary(temp_data)
```

```{r, echo = FALSE}
complete_data <- complete(temp_data,1)
complete_eval_data <- complete(temp_eval_data,1)
```

```{r}
densityplot(temp_data)
```

### Analysis of Outliers

Several predictors contained outliers that contradicted existing baseball statistics or fell out of an "acceptable" range given the features inherent distribution. These features are:

- bat_h: The most hits by team in a season is 1783. Therefore, any values above 1,783 were replaced with the median for the predictor [(Source)](https://www.baseball-almanac.com/recbooks/hits_records_mlb_teams.shtml).

```{r, echo = FALSE}
replace_median <- median(complete_data$bat_h[complete_data$bat_h <= 1783])

complete_data$bat_h[complete_data$bat_h > 1783] <- replace_median

complete_eval_data$bat_h[complete_eval_data$bat_h > 1783] <- replace_median
```


- p_h: We could not find any suitable statistics from outside sources for this feature. However, we can apply interquartile outlier analysis. By analyzing a given feature, those datapoints which fall above or below an "acceptable" range can be identified given the features inherent distribution.


```{r, echo = FALSE}
Q1 <- quantile(complete_data$p_h, probs=.25)
Q3 <- quantile(complete_data$p_h, probs=.75)
iqr = Q3-Q1
upper_limit = Q3 + (iqr*1.5)
lower_limit = Q1 - (iqr*1.5)
replace_median <- median(complete_data$p_h[(complete_data$p_h < upper_limit) | (complete_data$p_h > lower_limit)])

complete_data$p_h[(complete_data$p_h > upper_limit) | (complete_data$p_h < lower_limit)] <- replace_median

complete_eval_data$p_h[(complete_eval_data$p_h > upper_limit) | (complete_eval_data$p_h < lower_limit)] <- replace_median

```


- p_so: The record for most strikeouts in a season is 1595. Anything above this should be removed or imputed [(Source)](https://www.baseball-almanac.com/recbooks/rb_strike2.shtml).

```{r, echo = FALSE}
replace_median <- median(complete_data$p_so[complete_data$p_so <= 1595])

complete_data$p_so[complete_data$p_so > 1595] <- replace_median

complete_eval_data$p_so[complete_eval_data$p_so > 1595] <- replace_median
```

- f_e: The record for most errors in a season is 886. Anything above this should be removed or imputed [(Source)](https://www.baseball-fever.com/forum/general-baseball/statistics-analysis-sabermetrics/2403-team-errors-in-a-season). 

```{r, echo = FALSE}
replace_median <- median(complete_data$f_e[complete_data$f_e <= 886])

complete_data$f_e[complete_data$f_e > 886] <- replace_median

complete_eval_data$f_e[complete_eval_data$f_e > 886] <- replace_median
```

- p_bb: We could not find any suitable statistics from outside sources for this feature. However, we can apply interquartile outlier analysis. By analyzing a given feature, those datapoints which fall above or below an "acceptable" range can be identified given the features inherent distribution.

```{r, echo = FALSE}
Q1 <- quantile(complete_data$p_bb, probs=.25)
Q3 <- quantile(complete_data$p_bb, probs=.75)
iqr = Q3-Q1
upper_limit = Q3 + (iqr*1.5)
lower_limit = Q1 - (iqr*1.5)
replace_median <- median(complete_data$p_bb[(complete_data$p_bb < upper_limit) | (complete_data$p_bb > lower_limit)])

complete_data$p_bb[(complete_data$p_bb > upper_limit) | (complete_data$p_bb < lower_limit)] <- replace_median

complete_eval_data$p_bb[(complete_eval_data$p_bb > upper_limit) | (complete_eval_data$p_bb < lower_limit)] <- replace_median

```

#### Updated Distributions After Outlier Analysis and Imputing NA Values

```{r echo = FALSE}
ggplot(stack(complete_data), aes(x = ind, y = values, fill=ind)) +
  geom_boxplot() + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
```

< THIS LOOKS MUCH BETTER, THERE ARE STILL SOME PROBLEMS WITH FEATURES LIKE BAS_SB, F_E... WE CAN CONSIDER ADDRESSING THESE AT A LATER TIME (BUT WILL NOT DO IT!!!!!!)>

### Box-Cox Transformation for skewed variables

<Remind the reader which features had skews>

Try replacing the 0 values with really small values(1e-6) which will allow you to perform the Box-Cox transformation.

```{r}
complete_data[complete_data == 0] <- 1e-6

complete_eval_data[complete_eval_data == 0] <- 1e-6
```

After we use `powerTransform` to do the Box-Cox transformation, we than delete the original columns from `complete_data` using the `select` function from dplyr. Than use the `cbind` function to append the `transformed_data` to the `complete_data`. 

```{r}

skewed_vars <- "bat_3b, bas_sb, bas_cs, f_e"

lambdas <- powerTransform(eval(parse(text = paste("cbind(",skewed_vars,")", "~ 1"))), complete_data)

transformed_data <- bcPower(lambdas$y, coef(lambdas))

complete_data <- cbind(subset(complete_data, select = eval(parse(text = paste("-c(", skewed_vars, ")")))),
                       transformed_data)
```

```{r}

lambdas <- powerTransform(eval(parse(text = paste("cbind(",skewed_vars,")", "~ 1"))), complete_eval_data)

transformed_data <- bcPower(lambdas$y, coef(lambdas))
colnames(transformed_data) <- colnames(lambdas$y)

complete_eval_data <- cbind(subset(complete_eval_data, select = eval(parse(text = paste("-c(", skewed_vars, ")")))),transformed_data)
```


```{r echo = FALSE}
complete_data %>%
  keep(is.numeric) %>% 
  gather() %>% 
  ggplot(aes(value)) +
    facet_wrap(~ key, scales = "free") +
    geom_histogram()
```

### Dealing with Bimodal Variables

<@Peter please write an explanation for your approach>

```{r}
# Finds where two histograms intersect
histogram_intersection <- function(mu_1, mu_2, sigma_1, sigma_2){
  if (sigma_1 == sigma_2) stop('Both Sigmas are the same. Get 1/0')
  (mu_2*(sigma_1^2) - sigma_2*(mu_1*sigma_2 + sigma_1*sqrt((mu_1 - mu_2)^2 + 2*(sigma_1^2 -      sigma_2^2)*log(sigma_1/sigma_2))))/(sigma_1^2 - sigma_2^2)
}

# Fits two histograms to df[,bimodal_var] where `bimodal_var` is a bimodal
# variable. Than finds the point where the two histograms intersects. This
# value is returned as `cutoff`
create_bimodal_cutoff <- function(bimodal_var, df){
  bimodal_var_data <-  df[,bimodal_var]
  mixmdl = normalmixEM(bimodal_var_data)
  
  mu_1 = mixmdl$mu[1]
  mu_2 = mixmdl$mu[2]
  
  sigma_1 = mixmdl$sigma[1]
  sigma_2 = mixmdl$sigma[2]
  
  cutoff <- histogram_intersection(mu_1, mu_2, sigma_1, sigma_2)
  
  plot(mixmdl,which=2)
  lines(density(bimodal_var_data), lty=2, lwd=2)
  abline(v = cutoff)
  return(cutoff)
}

# Creates a dummy variable where any values for df[,bimodal_var] below `cutoff`
# are given a 1, and any values above are given a 0. Since these are dummy
# variables, they are converted from `numeric` to `factor`s using the `factor`
# function.
append_bimodal_dummy_var <- function(cutoff, bimodal_var, df){
  df[,paste("bi", bimodal_var, sep = "_")] <- factor((df[,bimodal_var] < cutoff) * 1)
  return(df)
}

# Creates dummy variables based on bimodal data. 
create_bimodal_dummy_var <- function(bimodal_var, df){
  cutoff <- create_bimodal_cutoff(bimodal_var, df)
  df <- append_bimodal_dummy_var(cutoff, bimodal_var, df)
  return(df)
}

```

```{r}
for (bimodal_var in c("bat_so", "p_hr", "bat_hr")){
  complete_data <- create_bimodal_dummy_var(bimodal_var, complete_data)
}
```

```{r eval=FALSE}
ggplot(data = melt(complete_data, "target"), aes(value)) +
  geom_histogram() +
  facet_wrap(.~variable, scales = "free")
```


--------------------------- PETER TO CORRECT / CREATE FUNCTION


### Add SABER analysis

Saber Model

Sabermetrics has become the rage in baseball, actually popularized by Billy Beane and the data set we are exploring. As a result, we built a model that centers around one of these advance analytics known as BsR or base runs. This statistic (designed by David Smyth in the 1990’s) estimates the amount of runs a team SHOULD score, adding an intriguing element to a data set which does not include runs (see http://tangotiger.net/wiki_archive/Base_Runs.html for more information). The formula For BsR is as follows:

BSR = A*B/(B+A)+C where:

A = TEAM_BATTING_1B + TEAM_BATTING_2B + TEAM_BATTING_3B + TEAM_BATTING_BB

B = 1.02(1.4TEAM_TOTAL_BASES -0.6TEAM_BATTING_H + 0.1TEAM_BATTING_BB)

C = TEAM_BATTING_HR

Since we eliminate the value of TEAM_BATTING_H, we sum up singles, doubles, triples and home runs in the actual code, and the approach for TEAM_TOTAL_BASES is described in model 2. The data for BSR exhibit a fairly normal distribution.


```{r}
complete_data$bat_1b <- complete_data$bat_h - complete_data$bat_2b - complete_data$bat_3b - complete_data$bat_hr
complete_data$total_bases <- complete_data$bat_1b + 2*complete_data$bat_2b + 3*complete_data$bat_3b + 4*complete_data$bat_hr


A <- complete_data$bat_h
B <- 1.02*(1.4*complete_data$total_bases -0.6*complete_data$bat_h + 0.1*complete_data$bat_bb)
C <- complete_data$bat_hr

complete_data$saber <- A*B/(B+A)+C
```

```{r}
complete_eval_data$bat_1b <- complete_eval_data$bat_h - complete_eval_data$bat_2b - complete_eval_data$bat_3b - complete_eval_data$bat_hr

complete_eval_data$total_bases <- complete_eval_data$bat_1b + 2*complete_eval_data$bat_2b + 3*complete_eval_data$bat_3b + 4*complete_eval_data$bat_hr


A <- complete_eval_data$bat_h
B <- 1.02*(1.4*complete_eval_data$total_bases -0.6*complete_eval_data$bat_h + 0.1*complete_eval_data$bat_bb)
C <- complete_eval_data$bat_hr

complete_eval_data$saber <- A*B/(B+A)+C
```

```{r}
write.csv(complete_data, "../data/final_clean_train.csv")
```

```{r}
write.csv(complete_eval_data, "../data/final_clean_eval.csv")
```


